import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

import nltk
import re
import string
#from itertools import chain
from collections import Counter
from wordcloud import WordCloud
import gensim
from scipy.stats.morestats import yeojohnson_normplot


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from sklearn import metrics
from sklearn.metrics import confusion_matrix


import nltk
nltk.download('stopwords')


fake = pd.read_csv('https://raw.githubusercontent.com/Dheeraj0725/Fake-news-detection/main/Dataset/Fake.csv')



fake.head(3)

fake.columns

fake['subject'].value_counts()

pd.isna(fake).sum()

fake_dt_fig = px.pie(fake,names='subject')
fake_dt_fig.show()

px.histogram(fake, x="subject", barmode="group")


commonFake = Counter(" ".join(fake["text"]).split()).most_common(100)
commonFake


list_1 = []
list_2 = []

for i in commonFake:
      X, y = i
      list_1.append(X)
      list_2.append(y)

list_1


x=list_1
y=list_2

fig = px.bar(x,x,y,height=400)
fig.update_layout(yaxis_range=[10000,500000])
fig.show()

l1 = ['trump','president','people','obama','donald','clinton','hillary','white','news','us','being','against','state','don','media','american']
l2 = ['72126','23051','22775','17688','17618','16496','14175','12971','12711','12340','12308','11495','11219','9905','9895','9886']

fig = px.bar(l1,l1,l2,height=400)
fig.update_layout(yaxis={'categoryorder':'total descending'})
fig.show()

text = ' '.join(fake['text'].tolist())

wordcloud = WordCloud(width = 1920, height = 1080).generate(text)
fig = plt.figure(
    figsize = (20, 20),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

real = pd.read_csv('https://raw.githubusercontent.com/Dheeraj0725/Fake-news-detection/main/Dataset/True.csv')

real.head(3)

pd.isna(real).sum()

text = ' '.join(real['text'].tolist())
true_dt_fig = px.pie(real,names='subject')
true_dt_fig.show()

px.histogram(real, x="subject", barmode="group")

commonReal = Counter(" ".join(real["text"]).split()).most_common(100)
commonReal

list_true1 = []
list_true2 = []

for i in commonReal:
      X, y = i
      list_true1.append(X)
      list_true2.append(y)

x=list_true1
y=list_true2

fig = px.bar(x,x,y)
fig.show()

l1 = ['US','Trump','President','United','Government','Republican','House','People','Trumps','Against','Donald','Former','State','He','North','States','White','Percent','Presidental','Campaign','Senate','Democratic','First','Since']
l2 = ['38271','36461','17112','15030','14667','14487','13622','11702','11582','10869','10263','9904','9057','9050','9035','8687','8472','7969','7850','7846','7702','7645','7605','7376']

fig = px.bar(l1,l1,l2,height=400)
fig.update_layout(yaxis={'categoryorder':'total descending'})
fig.show()

wordcloud = WordCloud(width = 1920, height = 1080).generate(text)
fig = plt.figure(
    figsize = (20, 20),
    facecolor = 'k',
    edgecolor = 'k')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

real.text.values

unknown_publishers = []
for index, row in enumerate(real.text.values):
  try:
    record = row.split('-', maxsplit=1)
    record[1]
    assert(len(record[0])<120)
  except:
    unknown_publishers.append(index)

len(unknown_publishers)

real.iloc[unknown_publishers]

real.iloc[unknown_publishers].text

real.iloc[8970]

publisher = []
tmp_text = []

for index, row in enumerate(real.text.values):
  if index in unknown_publishers:
    tmp_text.append(row)
    publisher.append('Unknown')
  else:
    record = row.split('-', maxsplit=1)
    publisher.append(record[0].strip())
    tmp_text.append(record[1].strip())

real['publisher']=publisher
real['text'] = tmp_text


real.head()

real.shape

empty_fake_index = [index for index, text in enumerate(fake.text.tolist()) if str(text).strip()==""]

fake.iloc[empty_fake_index]

real['text'] = real['title'] + " " + real['text']
fake['text'] = fake['title'] + " " + fake['text']

real['text'] = real['text'].apply(lambda x: str(x).lower())
fake['text'] = fake['text'].apply(lambda x: str(x).lower())


real['class'] = 1
fake['class'] = 0

real = real[['text', 'class']]
fake = fake[['text', 'class']]

data = real.append(fake, ignore_index=True)

data.sample(5)


stop_words = set(stopwords.words('english'))
to_remove = ['•', '!', '"', '#', '”', '“', '$', '%', '&', "'", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']
stop_words.update(to_remove)
print('Number of stopwords:', len(stop_words))

def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+', '', text)
    text = re.sub('\[[^]]*\]', '', text)
    text = (" ").join([word for word in text.split() if not word in stop_words])
    text = "".join([char for char in text if not char in to_remove])
    return text

data['text'] = data['text'].apply(clean_text)

data.head()

y = data['class'].values

X = [d.split() for d in data['text'].tolist()]

type(X[0])

print(X[0])

DIM = 100

w2v_model = gensim.models.Word2Vec(sentences=X, size=DIM, window=10, min_count=1)

len(w2v_model.wv.vocab)

w2v_model.wv.vocab

w2v_model.wv['science']

w2v_model.wv.most_similar('science')


tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

X

tokenizer.word_index

px.histogram([len(x) for x in X])

nos = np.array([len(x) for x in X])
len(nos[nos>1000])

maxlen = 1000
X = pad_sequences(X, maxlen=maxlen)

vocab_size = len(tokenizer.word_index) + 1
vocab = tokenizer.word_index

from gensim.models.fasttext import Vocab
def get_weight_matrix(model):
  weight_matrix = np.zeros((vocab_size, DIM))

  for word, i in vocab.items():
    weight_matrix[i] = model.wv[word]

  return weight_matrix


embedding_vectors = get_weight_matrix(w2v_model)

embedding_vectors.shape

model = Sequential()
model.add(Embedding(vocab_size, output_dim=DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))
model.add(LSTM(units=128))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])


model.summary()

X_train, X_test, y_train, y_test = train_test_split(X,y)

history = model.fit(X_train, y_train, validation_split=0.3, epochs=6)

y_pred = (model.predict(X_test) >= 0.5).astype(int)

accuracy_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

X_test



def plot_loss_epochs(history):
    epochs = np.arange(1,len(history.history['acc']) + 1,1)
    train_acc = history.history['acc']
    train_loss = history.history['loss']
    val_acc = history.history['val_acc']
    val_loss = history.history['val_loss']

    fig , ax = plt.subplots(1,2, figsize=(7,3))
    ax[0].plot(epochs , train_acc , '.-' , label = 'Train Accuracy')
    ax[0].plot(epochs , val_acc , '.-' , label = 'Validation Accuracy')
    ax[0].set_title('Train & Validation Accuracy')
    ax[0].legend()
    ax[0].set_xlabel("Epochs")
    ax[0].set_ylabel("Accuracy")

    ax[1].plot(epochs , train_loss , '.-' , label = 'Train Loss')
    ax[1].plot(epochs , val_loss , '.-' , label = 'Validation Loss')
    ax[1].set_title('Train & Validation Loss')
    ax[1].legend()
    ax[1].set_xlabel("Epochs")
    ax[1].set_ylabel("Loss")
    fig.tight_layout()
    fig.show()
    
plot_loss_epochs(history)


cm = confusion_matrix(y_test,y_pred)

confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
cm_display.plot()
plt.show()


def predictData(sourceData):
  sourceData = tokenizer.texts_to_sequences(sourceData)
  sourceData = pad_sequences(sourceData, maxlen=maxlen)
  final_value = (model.predict(sourceData)>=0.5).astype(int)

  if final_value == 0:
    print("FAKE NEWS DETECTED")
  else:
    print("REAL NEWS")



predictData(['A US official confirmed to CNN that Blome was summoned by Pakistan’s foreign ministry following Biden’s remarks. Those remarks frustrated US diplomats in the region, the US official said.'])


predictData(['elon musk buys oracle, puts java out of use and shuts down company'])







